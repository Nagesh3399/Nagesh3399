{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":73231,"databundleVersionId":8365361,"sourceType":"competition"},{"sourceId":8582111,"sourceType":"datasetVersion","datasetId":5132525},{"sourceId":8582323,"sourceType":"datasetVersion","datasetId":5132653},{"sourceId":8582325,"sourceType":"datasetVersion","datasetId":5132655},{"sourceId":8592406,"sourceType":"datasetVersion","datasetId":5139786},{"sourceId":8614727,"sourceType":"datasetVersion","datasetId":5155985},{"sourceId":8756042,"sourceType":"datasetVersion","datasetId":5260088},{"sourceId":8756059,"sourceType":"datasetVersion","datasetId":5260102},{"sourceId":8756161,"sourceType":"datasetVersion","datasetId":5260177},{"sourceId":11413,"sourceType":"modelInstanceVersion","modelInstanceId":6206},{"sourceId":27772,"sourceType":"modelInstanceVersion","modelInstanceId":15753}],"dockerImageVersionId":30733,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Submission Notebook\n","metadata":{}},{"cell_type":"code","source":"%pip install /kaggle/input/bitsandbytes-0-43-1/bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl\n%pip install /kaggle/input/transformer-4-41-2/transformers-4.41.2-py3-none-any.whl\n%pip install /kaggle/input/accelerate-0-30-1/accelerate-0.30.1-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2024-06-27T18:38:02.967943Z","iopub.execute_input":"2024-06-27T18:38:02.969072Z","iopub.status.idle":"2024-06-27T18:39:50.933189Z","shell.execute_reply.started":"2024-06-27T18:38:02.969030Z","shell.execute_reply":"2024-06-27T18:39:50.931905Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Processing /kaggle/input/bitsandbytes-0-43-1/bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes==0.43.1) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes==0.43.1) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes==0.43.1) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes==0.43.1) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes==0.43.1) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes==0.43.1) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes==0.43.1) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes==0.43.1) (2024.3.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes==0.43.1) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes==0.43.1) (1.3.0)\nInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.43.1\nNote: you may need to restart the kernel to use updated packages.\nProcessing /kaggle/input/transformer-4-41-2/transformers-4.41.2-py3-none-any.whl\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.41.2) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.41.2) (0.23.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.41.2) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.41.2) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.41.2) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.41.2) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.41.2) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers==4.41.2) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.41.2) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.41.2) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.2) (2024.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.2) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.41.2) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.41.2) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.41.2) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.41.2) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.41.2) (2024.2.2)\ntransformers is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\nNote: you may need to restart the kernel to use updated packages.\nProcessing /kaggle/input/accelerate-0-30-1/accelerate-0.30.1-py3-none-any.whl\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.30.1) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.30.1) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.30.1) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.30.1) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.30.1) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate==0.30.1) (0.23.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.30.1) (0.4.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate==0.30.1) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30.1) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30.1) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30.1) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30.1) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30.1) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30.1) (2024.3.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.30.1) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.30.1) (4.66.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.30.1) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.30.1) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.30.1) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.30.1) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.30.1) (2024.2.2)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.30.1) (1.3.0)\naccelerate is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig,TrainingArguments\nfrom IPython.display import Markdown, display\nimport subprocess\nimport re\n","metadata":{"execution":{"iopub.status.busy":"2024-06-27T18:39:50.935189Z","iopub.execute_input":"2024-06-27T18:39:50.935517Z","iopub.status.idle":"2024-06-27T18:39:57.513983Z","shell.execute_reply.started":"2024-06-27T18:39:50.935486Z","shell.execute_reply":"2024-06-27T18:39:57.512925Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# class MathModel:\n#     def quant(self):\n#         modelName0 = \"/kaggle/input/codegemma/transformers/7b/1\"\n#         bnbConfig = BitsAndBytesConfig(\n#             load_in_4bit = True,\n#             bnb_4bit_quant_type=\"nf4\",\n#             bnb_4bit_compute_dtype=torch.bfloat16,\n#         )\n#         model0 = AutoModelForCausalLM.from_pretrained(\n#             modelName0,\n#             device_map = \"auto\",\n#             quantization_config=bnbConfig\n#         )\n#         tokenizer = AutoTokenizer.from_pretrained(modelName0 )\n        \n        \n        \n#         modelName1 = \"/kaggle/input/gemma/transformers/7b/2\"\n#         bnbConfig = BitsAndBytesConfig(\n#             load_in_4bit = True,\n#             bnb_4bit_quant_type=\"nf4\",\n#             bnb_4bit_compute_dtype=torch.bfloat16,\n#         )\n#         model1 = AutoModelForCausalLM.from_pretrained(\n#             modelName1,\n#             device_map = \"auto\",\n#             quantization_config=bnbConfig\n#         )\n#         tokenizer = AutoTokenizer.from_pretrained(modelName1)\n        \n        \n        \n#     def code(self, c):\n#         with open(\"code.py\", 'w') as file:\n#             file.write(code)\n#         result = subprocess.run(['python', 'code.py'], capture_output=True, text=True)\n#         output= result.stdout\n#         error_output = result.stderr\n#         if error_output:\n#             output=0\n#         ans=predict2(output)\n        \n            \n#         return ans\n        \n\n#     def predict(self, x):\n#         system =  \"You are a skilled software engineer who consistently produces high-quality Python code.\"\n#         user = f\"Write a Python code to solve the question ,{x}\"\n#         prompt = f\"System: {system} \\n User: {user} \\n AI: \"\n#         inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n#         outputs = model0.generate(**inputs, max_length=2000, num_return_sequences=1)\n#         text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n#         Markdown(text.split(\"AI:\")[1])\n#         o1=text.split(\"AI:\")[1]\n#         o2=predict1(c)\n#         ans=code(o2)\n#         ans=ans%1000\n        \n#         return ans\n\n        \n#     def predict1(self, c):\n#         system =  \"You are a skilled software engineer who is capable of understanding high-quality Python code.\"\n#         user = f\"Extract only the python code from the following input and if the python code is making use of more test cases limit it to the 15 test case: ,{c}\"\n#         prompt = f\"System: {system} \\n User: {user} \\n AI: \"\n#         inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n#         outputs = model0.generate(**inputs, max_length=2000, num_return_sequences=1)\n#         text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n#         Markdown(text.split(\"AI:\")[1])\n#         o3=text.split(\"AI:\")[1]\n    \n#         return o3\n    \n#     def predict2(self, o):\n#         system =  \"You are a skilled Text analyzer and capable of differentiating between numeric and other characters.\"\n#         user = f\"Extract only the numeric value present in the following text:{c}\"\n#         prompt = f\"System: {system} \\n User: {user} \\n AI: \"\n#         inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n#         outputs = model1.generate(**inputs, max_length=2000, num_return_sequences=1)\n#         text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n#         Markdown(text.split(\"AI:\")[1])\n#         o4=text.split(\"AI:\")[1]\n    \n#         return o4\n    \n        \n    \n    ","metadata":{"execution":{"iopub.status.busy":"2024-06-27T18:39:57.515293Z","iopub.execute_input":"2024-06-27T18:39:57.515810Z","iopub.status.idle":"2024-06-27T18:39:57.524499Z","shell.execute_reply.started":"2024-06-27T18:39:57.515774Z","shell.execute_reply":"2024-06-27T18:39:57.523471Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# model = MathModel()\n# model.quant()","metadata":{"execution":{"iopub.status.busy":"2024-06-27T18:39:57.527383Z","iopub.execute_input":"2024-06-27T18:39:57.527711Z","iopub.status.idle":"2024-06-27T18:39:57.540800Z","shell.execute_reply.started":"2024-06-27T18:39:57.527684Z","shell.execute_reply":"2024-06-27T18:39:57.539908Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# import aimo\n\n# env = aimo.make_env()\n# iter_test = env.iter_test()","metadata":{"execution":{"iopub.status.busy":"2024-06-27T18:39:57.542180Z","iopub.execute_input":"2024-06-27T18:39:57.542482Z","iopub.status.idle":"2024-06-27T18:39:57.552015Z","shell.execute_reply.started":"2024-06-27T18:39:57.542459Z","shell.execute_reply":"2024-06-27T18:39:57.550789Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# for test, train in iter_test:\n#     train['answer'] = model.predict(test['problem'])\n#     env.predict(train)\n#     print(test)\n#     print(train, '\\n')","metadata":{"execution":{"iopub.status.busy":"2024-06-27T18:39:57.553177Z","iopub.execute_input":"2024-06-27T18:39:57.553457Z","iopub.status.idle":"2024-06-27T18:39:57.561663Z","shell.execute_reply.started":"2024-06-27T18:39:57.553433Z","shell.execute_reply":"2024-06-27T18:39:57.560803Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# for test, sample_submission in iter_test:\n#     sample_submission['answer'] = model.predict(test['problem'])\n#     env.predict(sample_submission)\n#     print(test)\n#     print(sample_submission, '\\n')","metadata":{"execution":{"iopub.status.busy":"2024-06-27T18:39:57.562744Z","iopub.execute_input":"2024-06-27T18:39:57.563071Z","iopub.status.idle":"2024-06-27T18:39:57.570845Z","shell.execute_reply.started":"2024-06-27T18:39:57.563042Z","shell.execute_reply":"2024-06-27T18:39:57.569980Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"\nclass MathModel:\n    def __init__(self):\n        self.model0 = None\n        self.tokenizer0 = None\n        self.model1 = None\n        self.tokenizer1 = None\n\n    def quant(self):\n        modelName0 = \"/kaggle/input/codegemma/transformers/7b/1\"\n        bnbConfig = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.bfloat16,\n        )\n        self.model0 = AutoModelForCausalLM.from_pretrained(\n            modelName0,\n            device_map=\"auto\",\n            quantization_config=bnbConfig\n        )\n        self.tokenizer0 = AutoTokenizer.from_pretrained(modelName0)\n\n        modelName1 = \"/kaggle/input/gemma/transformers/7b/2\"\n        bnbConfig = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.bfloat16,\n        )\n        self.model1 = AutoModelForCausalLM.from_pretrained(\n            modelName1,\n            device_map=\"auto\",\n            quantization_config=bnbConfig\n        )\n        self.tokenizer1 = AutoTokenizer.from_pretrained(modelName1)\n\n    def code(self, c):\n        with open(\"code.py\", 'w') as file:\n            file.write(c)\n        result = subprocess.run(['python', 'code.py'], capture_output=True, text=True)\n        output = result.stdout\n        error_output = result.stderr\n        if error_output:\n            output =0\n        ans = self.predict2(output)\n      \n        return ans\n\n    def predict(self, x):\n        system = \"You are a skilled software engineer who consistently produces high-quality Python code.\"\n        user = f\"Write a Python code to solve the question however if the code is making use of more number of test cases limit it to 15 test cases ,{x}\"\n        prompt = f\"System: {system} \\n User: {user} \\n AI: \"\n        inputs = self.tokenizer0(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n        outputs = self.model0.generate(**inputs, max_new_tokens=2500, num_return_sequences=1)\n        text = self.tokenizer0.decode(outputs[0], skip_special_tokens=True)\n        o1 = text.split(\"AI:\")[1]\n        o2 = self.predict1(o1)\n        ans = self.code(o2)\n                \n        try:\n            if isinstance(ans, list):\n                ans_str = \".\".join(map(str, ans))  # Join integers with '.' as the decimal separator\n                ans = float(ans_str)\n            else:\n                ans = float(ans)\n            ans = ans % 1000\n        except (ValueError, TypeError) as e:\n            print(f\"Error converting answer to float: {e}\")\n            ans = 0.0\n            \n        return ans\n\n    def predict1(self, c):\n        system = \"You are a skilled software engineer who is capable of understanding high-quality Python code.\"\n        user = f\"Extract only the python code from the following input and if the python code is making use of more test cases limit it to the 15 test case: ,{c}\"\n        prompt = f\"System: {system} \\n User: {user} \\n AI: \"\n        inputs = self.tokenizer0(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n        outputs = self.model0.generate(**inputs, max_new_tokens =2500, num_return_sequences=1)\n        text = self.tokenizer0.decode(outputs[0], skip_special_tokens=True)\n        o3 = text.split(\"AI:\")[1]\n    \n        return o3\n\n    def predict2(self, o):\n        system = \"You are a skilled Text analyzer and capable of differentiating between numeric and other characters.\"\n        user = f\"Extract only the numeric value present in the following text:{o}\"\n        prompt = f\"System: {system} \\n User: {user} \\n AI: \"\n        inputs = self.tokenizer1(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n        outputs = self.model1.generate(**inputs, max_new_tokens=2500, num_return_sequences=1)\n        text = self.tokenizer1.decode(outputs[0], skip_special_tokens=True)\n        o4 = text.split(\"AI:\")[1]\n        numeric_values = re.findall(r'\\d+\\.\\d+|\\d+', o4)\n        \n        # Convert extracted values to numeric format\n        numeric_values = [float(value) if '.' in value else int(value) for value in numeric_values]\n        \n        return numeric_values\n        \n        \n\n    \n        \n","metadata":{"execution":{"iopub.status.busy":"2024-06-27T18:39:57.572149Z","iopub.execute_input":"2024-06-27T18:39:57.572469Z","iopub.status.idle":"2024-06-27T18:39:57.594344Z","shell.execute_reply.started":"2024-06-27T18:39:57.572445Z","shell.execute_reply":"2024-06-27T18:39:57.593386Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"model = MathModel()\nmodel.quant()","metadata":{"execution":{"iopub.status.busy":"2024-06-27T18:39:57.595633Z","iopub.execute_input":"2024-06-27T18:39:57.595898Z","iopub.status.idle":"2024-06-27T18:43:53.754590Z","shell.execute_reply.started":"2024-06-27T18:39:57.595875Z","shell.execute_reply":"2024-06-27T18:43:53.753227Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\nGemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n`config.hidden_activation` if you want to override this behaviour.\nSee https://github.com/huggingface/transformers/pull/29402 for more details.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46856ce8222748a7922a9b340745a649"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e937f2ec70b4d61bc4dcd6ed65d3ab6"}},"metadata":{}}]},{"cell_type":"code","source":"import aimo\n\nenv = aimo.make_env()\niter_test = env.iter_test()","metadata":{"execution":{"iopub.status.busy":"2024-06-27T18:43:53.758574Z","iopub.execute_input":"2024-06-27T18:43:53.758921Z","iopub.status.idle":"2024-06-27T18:43:54.749808Z","shell.execute_reply.started":"2024-06-27T18:43:53.758890Z","shell.execute_reply":"2024-06-27T18:43:54.748679Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"for test, sample_submission in iter_test:\n    sample_submission['answer'] = model.predict(test['problem'])\n    env.predict(sample_submission)\n    print(test)\n    print(sample_submission, '\\n')","metadata":{"execution":{"iopub.status.busy":"2024-06-27T18:43:54.751214Z","iopub.execute_input":"2024-06-27T18:43:54.751865Z","iopub.status.idle":"2024-06-27T19:07:40.098183Z","shell.execute_reply.started":"2024-06-27T18:43:54.751825Z","shell.execute_reply":"2024-06-27T19:07:40.096972Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n","output_type":"stream"},{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n2024-06-27 18:43:58.043094: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-27 18:43:58.043244: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-27 18:43:58.176975: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"name":"stdout","text":"       id         problem\n0  000aaa  What is $1-1$?\n       id  answer\n0  000aaa     0.1 \n\n       id               problem\n0  111bbb  What is $0\\times10$?\n       id  answer\n0  111bbb     0.1 \n\n       id                 problem\n0  222ccc  Solve $4+x=4$ for $x$.\n       id  answer\n0  222ccc     0.1 \n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import pandas as pd\n# tr_data=pd.read_csv(\"/kaggle/input/ai-mathematical-olympiad-prize/train.csv\")\n# train=pd.DataFrame(tr_data)\n# te_data=pd.read_csv(\"/kaggle/input/ai-mathematical-olympiad-prize/test.csv\")\n# test=pd.DataFrame(te_data)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-27T19:07:40.099829Z","iopub.execute_input":"2024-06-27T19:07:40.100554Z","iopub.status.idle":"2024-06-27T19:07:40.105181Z","shell.execute_reply.started":"2024-06-27T19:07:40.100506Z","shell.execute_reply":"2024-06-27T19:07:40.104193Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# %pip install /kaggle/input/bitsandbytes-0-43-1/bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl\n# %pip install /kaggle/input/huggingface-hub-0-23-0/huggingface_hub-0.23.0-py3-none-any.whl\n# %pip install /kaggle/input/transformer-4-41-2/transformers-4.41.2-py3-none-any.whl\n# %pip install /kaggle/input/tokenizers-0-19-0/tokenizers-0.19.0-pp310-pypy310_pp73-musllinux_1_1_x86_64.whl\n# %pip install /kaggle/input/accelerate-0-30-1/accelerate-0.30.1-py3-none-any.whl\n# %pip install /kaggle/input/peft-0-11-1/peft-0.11.1-py3-none-any.whl\n# %pip install /kaggle/input/datasets-2-19-2/datasets-2.19.2-py3-none-any.whl\n# %pip install /kaggle/input/requests-2-32-1/requests-2.32.1-py3-none-any.whl\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-27T19:07:40.106560Z","iopub.execute_input":"2024-06-27T19:07:40.106881Z","iopub.status.idle":"2024-06-27T19:07:40.115956Z","shell.execute_reply.started":"2024-06-27T19:07:40.106854Z","shell.execute_reply":"2024-06-27T19:07:40.115174Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# %pip install /kaggle/input/bitsandbytes-0-43-1/bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl\n# %pip install /kaggle/input/transformer-4-41-2/transformers-4.41.2-py3-none-any.whl\n# %pip install /kaggle/input/accelerate-0-30-1/accelerate-0.30.1-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2024-06-27T19:07:40.117192Z","iopub.execute_input":"2024-06-27T19:07:40.117568Z","iopub.status.idle":"2024-06-27T19:07:40.126854Z","shell.execute_reply.started":"2024-06-27T19:07:40.117534Z","shell.execute_reply":"2024-06-27T19:07:40.126031Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# import torch\n# from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig,TrainingArguments\n# from IPython.display import Markdown, display","metadata":{"execution":{"iopub.status.busy":"2024-06-27T19:07:40.128033Z","iopub.execute_input":"2024-06-27T19:07:40.128310Z","iopub.status.idle":"2024-06-27T19:07:40.136446Z","shell.execute_reply.started":"2024-06-27T19:07:40.128286Z","shell.execute_reply":"2024-06-27T19:07:40.135613Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"\n# modelName = \"/kaggle/input/codegemma/transformers/7b/1\"\n\n# bnbConfig = BitsAndBytesConfig(\n#     load_in_4bit = True,\n#     bnb_4bit_quant_type=\"nf4\",\n#     bnb_4bit_compute_dtype=torch.bfloat16,\n# )\n\n# model = AutoModelForCausalLM.from_pretrained(\n#     modelName,\n#     device_map = \"auto\",\n#     quantization_config=bnbConfig\n# )\n\n# tokenizer = AutoTokenizer.from_pretrained(modelName)","metadata":{"execution":{"iopub.status.busy":"2024-06-27T19:07:40.137525Z","iopub.execute_input":"2024-06-27T19:07:40.137815Z","iopub.status.idle":"2024-06-27T19:07:40.145464Z","shell.execute_reply.started":"2024-06-27T19:07:40.137791Z","shell.execute_reply":"2024-06-27T19:07:40.144637Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"\n# system =  \"You are a skilled software engineer who consistently produces high-quality Python code.\"\n# user = \"Write a Python code to solve the question, Manisha got 91 marks in English, 83 in Science, 87 in Maths, 81 in Hindi and 95 in Socialogy. A student can get maximum 100 marks in each subject. What was the percentage of Manisha in this examination?\"\n\n# prompt = f\"System: {system} \\n User: {user} \\n AI: \"\n    \n# inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n\n# outputs = model.generate(**inputs, max_length=500, num_return_sequences=1)\n\n# text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Markdown(text.split(\"AI:\")[1])","metadata":{"execution":{"iopub.status.busy":"2024-06-27T19:07:40.146788Z","iopub.execute_input":"2024-06-27T19:07:40.147152Z","iopub.status.idle":"2024-06-27T19:07:40.159142Z","shell.execute_reply.started":"2024-06-27T19:07:40.147124Z","shell.execute_reply":"2024-06-27T19:07:40.158154Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# code=text.split(\"AI:\")[1]\n# print(code)","metadata":{"execution":{"iopub.status.busy":"2024-06-27T19:07:40.160352Z","iopub.execute_input":"2024-06-27T19:07:40.160681Z","iopub.status.idle":"2024-06-27T19:07:40.167818Z","shell.execute_reply.started":"2024-06-27T19:07:40.160644Z","shell.execute_reply":"2024-06-27T19:07:40.166950Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# with open(\"code.py\", 'w') as file:\n#     file.write(code)","metadata":{"execution":{"iopub.status.busy":"2024-06-27T19:07:40.168827Z","iopub.execute_input":"2024-06-27T19:07:40.169119Z","iopub.status.idle":"2024-06-27T19:07:40.178366Z","shell.execute_reply.started":"2024-06-27T19:07:40.169093Z","shell.execute_reply":"2024-06-27T19:07:40.177579Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# !python code.py","metadata":{"execution":{"iopub.status.busy":"2024-06-27T19:07:40.179390Z","iopub.execute_input":"2024-06-27T19:07:40.179649Z","iopub.status.idle":"2024-06-27T19:07:40.188947Z","shell.execute_reply.started":"2024-06-27T19:07:40.179627Z","shell.execute_reply":"2024-06-27T19:07:40.187827Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# import subprocess\n\n# # Run the external script and capture the output\n# result = subprocess.run(['python', 'code.py'], capture_output=True, text=True)\n\n# # Get the standard output (stdout)\n# output = result.stdout\n\n# # Optional: Get the standard error (stderr) if needed\n# error_output = result.stderr\n\n# # Print the captured output\n# print(output)\n\n# # Print the captured error output if any\n# if error_output:\n#     print(\"Captured Error Output:\")\n#     print(error_output)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-27T19:07:40.190129Z","iopub.execute_input":"2024-06-27T19:07:40.190435Z","iopub.status.idle":"2024-06-27T19:07:40.198148Z","shell.execute_reply.started":"2024-06-27T19:07:40.190411Z","shell.execute_reply":"2024-06-27T19:07:40.197420Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# from IPython.display import Markdown, display\n# system =  \"You are a text analyzer capable of differentiating text and numbers from the given text\"\n# user = f\"Give only the numerical output from {output}\"\n\n# prompt = f\"System: {system} \\n User: {user} \\n AI: \"\n    \n# inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n\n# outputs = model.generate(**inputs, max_length=5000, num_return_sequences=1)\n\n# text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Markdown(text.split(\"AI:\")[1])","metadata":{"execution":{"iopub.status.busy":"2024-06-27T19:07:40.199171Z","iopub.execute_input":"2024-06-27T19:07:40.199450Z","iopub.status.idle":"2024-06-27T19:07:40.206697Z","shell.execute_reply.started":"2024-06-27T19:07:40.199425Z","shell.execute_reply":"2024-06-27T19:07:40.205800Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# from IPython.display import Markdown, display\n# import re\n\n# # System and User Definitions\n# system = \"You are a text analyzer capable of differentiating text and numbers from the given text.\"\n# user_input = \"Manisha's percentage is 87.4\"\n# user = f\"Give only the numerical output from '{user_input}'\"\n\n# # Prompt Creation\n# prompt = f\"System: {system} \\nUser: {user} \\nAI: \"\n\n# # Tokenizing the Prompt\n# inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n\n# # Generating the Model's Response\n# outputs = model.generate(**inputs, max_length=5000, num_return_sequences=1)\n\n# # Decoding the Model's Response\n# text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# # Extracting Numerical Output\n# numerical_output = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", user_input)\n# numerical_output = numerical_output[0] if numerical_output else \"No numerical value found.\"\n\n# # Displaying the Result\n# display(Markdown(numerical_output))\n","metadata":{"execution":{"iopub.status.busy":"2024-06-27T19:07:40.207745Z","iopub.execute_input":"2024-06-27T19:07:40.207984Z","iopub.status.idle":"2024-06-27T19:07:40.219046Z","shell.execute_reply.started":"2024-06-27T19:07:40.207963Z","shell.execute_reply":"2024-06-27T19:07:40.218037Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from IPython.display import Markdown, display\n# system =  \"You are a skilled software engineer who consistently produces high-quality Python code.\"\n# user = \"Write a Python code to solve the question, Let $k, l > 0$ be parameters. The parabola $y = kx^2 - 2kx + l$ intersects the line $y = 4$ at two points $A$ and $B$. These points are distance 6 apart. What is the sum of the squares of the distances from $A$ and $B$ to the origin?\"\n\n# prompt = f\"System: {system} \\n User: {user} \\n AI: \"\n    \n# inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n\n# outputs = model.generate(**inputs, max_length=3000, num_return_sequences=1)\n\n# text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Markdown(text.split(\"AI:\")[1])","metadata":{"execution":{"iopub.status.busy":"2024-06-27T19:07:40.220417Z","iopub.execute_input":"2024-06-27T19:07:40.221122Z","iopub.status.idle":"2024-06-27T19:07:40.229448Z","shell.execute_reply.started":"2024-06-27T19:07:40.221090Z","shell.execute_reply":"2024-06-27T19:07:40.228502Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# code=text.split(\"AI:\")[1]\n# print(code)","metadata":{"execution":{"iopub.status.busy":"2024-06-27T19:07:40.230519Z","iopub.execute_input":"2024-06-27T19:07:40.231152Z","iopub.status.idle":"2024-06-27T19:07:40.241803Z","shell.execute_reply.started":"2024-06-27T19:07:40.231124Z","shell.execute_reply":"2024-06-27T19:07:40.240914Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# with open(\"code.py\", 'w') as file:\n#     file.write(code)","metadata":{"execution":{"iopub.status.busy":"2024-06-27T19:07:40.242837Z","iopub.execute_input":"2024-06-27T19:07:40.243136Z","iopub.status.idle":"2024-06-27T19:07:40.251178Z","shell.execute_reply.started":"2024-06-27T19:07:40.243113Z","shell.execute_reply":"2024-06-27T19:07:40.250397Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# !python code.py","metadata":{"execution":{"iopub.status.busy":"2024-06-27T19:07:40.252554Z","iopub.execute_input":"2024-06-27T19:07:40.252812Z","iopub.status.idle":"2024-06-27T19:07:40.259601Z","shell.execute_reply.started":"2024-06-27T19:07:40.252790Z","shell.execute_reply":"2024-06-27T19:07:40.258751Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# # Define your model\n# class Model:\n#     def predict(self, x):\n# from IPython.display import Markdown, display\n\n# system =  \"You are a skilled software engineer who consistently produces high-quality Python code.\"\n# user = f\"Write a Python code to solve the question,{x}\"\n# prompt = f\"System: {system} \\n User: {user} \\n AI: \"\n    \n# inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n\n# outputs = model.generate(**inputs, max_length=500, num_return_sequences=1)\n\n# text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Markdown(text.split(\"AI:\")[1])\n#         return answer\n\n    \n# model = Model()","metadata":{"execution":{"iopub.status.busy":"2024-06-27T19:07:40.265483Z","iopub.execute_input":"2024-06-27T19:07:40.265808Z","iopub.status.idle":"2024-06-27T19:07:40.270598Z","shell.execute_reply.started":"2024-06-27T19:07:40.265785Z","shell.execute_reply":"2024-06-27T19:07:40.269720Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# from IPython.display import Markdown, display\n\n# system =  \"You are a skilled software engineer who consistently produces high-quality Python code.\"\n# user = f\"Write a Python code to solve the question,{x}\"\n# prompt = f\"System: {system} \\n User: {user} \\n AI: \"\n    \n# inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n\n# outputs = model.generate(**inputs, max_length=500, num_return_sequences=1)\n\n# text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Markdown(text.split(\"AI:\")[1])\n","metadata":{"execution":{"iopub.status.busy":"2024-06-27T19:07:40.271838Z","iopub.execute_input":"2024-06-27T19:07:40.272703Z","iopub.status.idle":"2024-06-27T19:07:40.279464Z","shell.execute_reply.started":"2024-06-27T19:07:40.272670Z","shell.execute_reply":"2024-06-27T19:07:40.278715Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# code=text.split(\"AI:\")[1]\n","metadata":{"execution":{"iopub.status.busy":"2024-06-27T19:07:40.280741Z","iopub.execute_input":"2024-06-27T19:07:40.281336Z","iopub.status.idle":"2024-06-27T19:07:40.293393Z","shell.execute_reply.started":"2024-06-27T19:07:40.281297Z","shell.execute_reply":"2024-06-27T19:07:40.292505Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !python code.py","metadata":{"execution":{"iopub.status.busy":"2024-06-27T19:07:40.294544Z","iopub.execute_input":"2024-06-27T19:07:40.294899Z","iopub.status.idle":"2024-06-27T19:07:40.303547Z","shell.execute_reply.started":"2024-06-27T19:07:40.294873Z","shell.execute_reply":"2024-06-27T19:07:40.302217Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# # Set up the evaluation API\n# import aimo\n\n# env = aimo.make_env()\n# iter_test = env.iter_test()","metadata":{"execution":{"iopub.status.busy":"2024-06-27T19:07:40.304788Z","iopub.execute_input":"2024-06-27T19:07:40.305162Z","iopub.status.idle":"2024-06-27T19:07:40.312635Z","shell.execute_reply.started":"2024-06-27T19:07:40.305137Z","shell.execute_reply":"2024-06-27T19:07:40.311784Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# # Iterate through the test set and use the model make predictions\n# for test, sample_submission in iter_test:\n#     sample_submission['answer'] = model.predict(test['problem'])\n#     env.predict(sample_submission)\n#     print(test)\n#     print(sample_submission, '\\n')","metadata":{"execution":{"iopub.status.busy":"2024-06-27T19:07:40.314512Z","iopub.execute_input":"2024-06-27T19:07:40.315452Z","iopub.status.idle":"2024-06-27T19:07:40.322680Z","shell.execute_reply.started":"2024-06-27T19:07:40.315422Z","shell.execute_reply":"2024-06-27T19:07:40.321773Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# import torch\n# from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig, TrainingArguments\n# import pandas as pd\n# from IPython.display import Markdown, display\n\n# # Read the CSV file\n# df = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-prize/train.csv')\n\n# system = \"You are a skilled software engineer who consistently produces high-quality Python code.\"\n\n# # Loop through each question in the CSV\n# for index, row in df.iterrows():\n#     problem = row[1]  # Assuming the question is in the second column\n#     user = f\"Write a Python code to solve the question: {problem}\"\n#     prompt = f\"System: {system}\\nUser: {user}\\nAI: \"\n    \n#     inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n#     outputs = model.generate(**inputs, max_length=5000, num_return_sequences=1)\n#     text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n#     code = text.split(\"AI:\")[1].strip()\n    \n#     print(f\"Problem {index + 1}:\")\n#     print(code)\n    \n#     # Write the code to a file\n#     with open(\"code.py\", 'w') as file:\n#         file.write(code)\n    \n#     # Run the code and capture the output\n#     import subprocess\n#     result = subprocess.run(['python', 'code.py'], capture_output=True, text=True)\n    \n#     print(f\"Output for Problem {index + 1}:\")\n#     print(result.stdout)\n#     print(\"--------------------\")","metadata":{"execution":{"iopub.status.busy":"2024-06-27T19:07:40.323802Z","iopub.execute_input":"2024-06-27T19:07:40.324082Z","iopub.status.idle":"2024-06-27T19:07:40.333720Z","shell.execute_reply.started":"2024-06-27T19:07:40.324059Z","shell.execute_reply":"2024-06-27T19:07:40.332769Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"#  def tokenize_data(data):\n#         tokenized_data = tokenizer(\n#                data[\"problem_answer\"],  # Replace with the actual column name containing text\n#                padding=\"max_length\",  # Pad shorter sequences to a fixed length\n#                truncation=True,  # Truncate longer sequences\n#                return_tensors=\"pt\") # Convert data to PyTorch tensors (adjust based on your framework)\n#         return tokenized_data\n","metadata":{"execution":{"iopub.status.busy":"2024-06-27T19:07:40.334637Z","iopub.execute_input":"2024-06-27T19:07:40.335323Z","iopub.status.idle":"2024-06-27T19:07:40.345596Z","shell.execute_reply.started":"2024-06-27T19:07:40.335298Z","shell.execute_reply":"2024-06-27T19:07:40.344728Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# tr_data.columns.tolist()\n# # Assuming problem and answer might contain integers\n# tr_data[\"problem\"] = tr_data[\"problem\"].astype(str)\n# tr_data[\"answer\"] = tr_data[\"answer\"].astype(str)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-27T19:07:40.346798Z","iopub.execute_input":"2024-06-27T19:07:40.347104Z","iopub.status.idle":"2024-06-27T19:07:40.354829Z","shell.execute_reply.started":"2024-06-27T19:07:40.347079Z","shell.execute_reply":"2024-06-27T19:07:40.354048Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# tokenized_datasets = {}\n# tokenized_datasets[\"train\"] = tokenize_data(tr_data)\n# tokenized_datasets[\"test\"] = tokenize_data(te_data)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-27T19:07:40.356088Z","iopub.execute_input":"2024-06-27T19:07:40.356443Z","iopub.status.idle":"2024-06-27T19:07:40.366597Z","shell.execute_reply.started":"2024-06-27T19:07:40.356413Z","shell.execute_reply":"2024-06-27T19:07:40.365820Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from peft import LoraConfig, TaskType\n\n# peft_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=4, lora_alpha=32, lora_dropout=0.1)","metadata":{"execution":{"iopub.status.busy":"2024-06-27T19:07:40.367637Z","iopub.execute_input":"2024-06-27T19:07:40.367888Z","iopub.status.idle":"2024-06-27T19:07:40.376906Z","shell.execute_reply.started":"2024-06-27T19:07:40.367866Z","shell.execute_reply":"2024-06-27T19:07:40.376011Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# from peft import get_peft_model\n\n# model = get_peft_model(model, peft_config)\n# model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-06-27T19:07:40.378084Z","iopub.execute_input":"2024-06-27T19:07:40.378443Z","iopub.status.idle":"2024-06-27T19:07:40.386077Z","shell.execute_reply.started":"2024-06-27T19:07:40.378413Z","shell.execute_reply":"2024-06-27T19:07:40.385208Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# training_args = TrainingArguments(\n#     output_dir=\"/kaggle/working/\",\n#     learning_rate=1e-3,\n#     per_device_train_batch_size=32,\n#     per_device_eval_batch_size=32,\n#     num_train_epochs=2,\n#     weight_decay=0.01,\n#     evaluation_strategy=\"epoch\",\n#     save_strategy=\"epoch\",\n#     load_best_model_at_end=True,\n# )","metadata":{"execution":{"iopub.status.busy":"2024-06-27T19:07:40.387312Z","iopub.execute_input":"2024-06-27T19:07:40.387904Z","iopub.status.idle":"2024-06-27T19:07:40.394453Z","shell.execute_reply.started":"2024-06-27T19:07:40.387871Z","shell.execute_reply":"2024-06-27T19:07:40.393593Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## model.save_pretrained(\"output_dir\")","metadata":{}}]}